<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Random Variables and Expected Values Variance and covariance are expressed in terms of random variables and expected values Random Variable - A variable that takes on different values due to chance"><meta property="og:title" content="Covariance and Prinicpal Components"><meta property="og:description" content="Random Variables and Expected Values Variance and covariance are expressed in terms of random variables and expected values Random Variable - A variable that takes on different values due to chance"><meta property="og:type" content="website"><meta property="og:image" content="https://ik1g19.github.io/UoS-Notes/icon.png"><meta property="og:url" content="https://ik1g19.github.io/UoS-Notes/notes/Computer-Vision/Covariance-and-Prinicpal-Components/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="Covariance and Prinicpal Components"><meta name=twitter:description content="Random Variables and Expected Values Variance and covariance are expressed in terms of random variables and expected values Random Variable - A variable that takes on different values due to chance"><meta name=twitter:image content="https://ik1g19.github.io/UoS-Notes/icon.png"><meta name=twitter:site content="_jzhao"><title>Covariance and Prinicpal Components</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://ik1g19.github.io/UoS-Notes//icon.png><link href=https://ik1g19.github.io/UoS-Notes/styles.80333fa2099c0bee674efa435fde378c.min.css rel=stylesheet><link href=https://ik1g19.github.io/UoS-Notes/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://ik1g19.github.io/UoS-Notes/js/darkmode.fdbb50a651b073e7d85910aebde4469d.min.js></script>
<script src=https://ik1g19.github.io/UoS-Notes/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script async src=https://ik1g19.github.io/UoS-Notes/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://ik1g19.github.io/UoS-Notes/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://ik1g19.github.io/UoS-Notes/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://ik1g19.github.io/UoS-Notes/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://ik1g19.github.io/UoS-Notes/",fetchData=Promise.all([fetch("https://ik1g19.github.io/UoS-Notes/indices/linkIndex.4cd7744b37da745103d2e306cd2cd001.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://ik1g19.github.io/UoS-Notes/indices/contentIndex.a1e306a3e6a8cedcc8275a04564ca752.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://ik1g19.github.io/UoS-Notes",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://ik1g19.github.io/UoS-Notes",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/ik1g19.github.io\/UoS-Notes\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=ik1g19.github.io/UoS-Notes src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://ik1g19.github.io/UoS-Notes/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://ik1g19.github.io/UoS-Notes/>My UoS Notes</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Covariance and Prinicpal Components</h1><p class=meta>Last updated
Apr 11, 2023
<a href=https://github.com/ik1g19/notes/Computer%20Vision/Covariance%20and%20Prinicpal%20Components.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#basis>Basis</a></li><li><a href=#the-first-principal-axis>The First Principal Axis</a></li><li><a href=#the-second-principal-axis>The Second Principal Axis</a></li><li><a href=#the-third-principal-axis>The Third Principal Axis</a></li></ol><ol><li><a href=#ordering>Ordering</a></li></ol><ol><li><a href=#linear-transform>Linear Transform</a></li><li><a href=#pca>PCA</a><ol><li><a href=#pca-algorithm>PCA Algorithm</a></li></ol></li></ol><ol><li><a href=#making-it-invariant>Making it invariant</a></li><li><a href=#another-problem-is-the-size-of-the-featurevectors>Another problem is the size of the featurevectors</a></li></ol></nav></details></aside><a href=#random-variables-and-expected-values><h1 id=random-variables-and-expected-values><span class=hanchor arialabel=Anchor># </span>Random Variables and Expected Values</h1></a><p>Variance and covariance are expressed in terms of random variables and expected values
Random Variable - A variable that takes on different values due to chance</p><ul><li>The set of sample values from a single dimension of a featurespace can be considered to be a random variable
The expected value $E<input checked disabled type=checkbox> $ is the most likely value a random variable will take
We will assume the values an element of a feature can take are equally likely
So the expected value is just the mean value</li></ul><a href=#variance><h1 id=variance><span class=hanchor arialabel=Anchor># </span>Variance</h1></a><p>Variance $\sigma^2$ is the mean squared difference from the mean
Effectively a measure of how spread-out the data is
$$\sigma^2(x)=\frac{1}{n}\sum\limits_{i=1}^{n}(x_i-\mu)^2$$</p><a href=#covariance><h1 id=covariance><span class=hanchor arialabel=Anchor># </span>Covariance</h1></a><p>Covariance $\sigma(x,y)$ measures how two variables change together
It is technically $E[(x-E[x])(y-E[y])]$
$$\sigma(x,y)=\frac{1}{n}\sum\limits_{i=1}^{n}(x-\mu_x)(y-\mu_y)$$
The variance is the covariance when the two variables are the same (\sigma(x,x)=\sigma^2(x))
A covariance of 0 means the variables are uncorrelated</p><a href=#covariance-matrix><h1 id=covariance-matrix><span class=hanchor arialabel=Anchor># </span>Covariance Matrix</h1></a><p>Encodes how all possible pairs of dimensions in an n-dimensional data set vary together
$$\Sigma=\begin{bmatrix}
\sigma(X_1,X_1) & \sigma(X_1,X_2) & &mldr; & \sigma(X_1,X_n)\\ \sigma(X_2,X_1) & \sigma (X_2,X_2) & &mldr; & \sigma(X_2,X_n)\\ &mldr; & &mldr; & &mldr; & &mldr;\\ \sigma(X_n,X_1) & \sigma(X_n,X_2) & &mldr; & \sigma(X_n,X_n)
\end{bmatrix}$$
The covariance matrix is a square symmetric matrix
<img src=https://remnote-user-data.s3.amazonaws.com/Xpv_rK9URuvkjTvmpGlFKJi3i1z66oQ0zgbZm8O2mAkYZL8V-juD4R0jwgJvfbicBP-Q7FSuQVoMZaduhkm6MmdzlCCAow0NwAc99ZJW_8kgiUKUNdPmxhKaSOn5nzkd.png width=400 alt=|400></p><a href=#mean-centring><h1 id=mean-centring><span class=hanchor arialabel=Anchor># </span>Mean Centring</h1></a><p>The process of computing the mean (across each dimension independently) of a set of vectors, and then subtracting the mean vector from every vector in the set
All the vectors will be translated so their average position is the origin
<img src=https://remnote-user-data.s3.amazonaws.com/0WtNAd-YtLPMBpuSI9ogei6eEVHvKyN1QbCYu9FsZm6tHM5pcvc2s3b9de9G7I4LhzIZyq6jHZ64HZ_pMTt5uMONEm9-Upc2geFKwsznX4hT0TNF5ZXedMMUe16sXIzY.png width=400 alt=|400></p><a href=#covariance-matrix-properties><h1 id=covariance-matrix-properties><span class=hanchor arialabel=Anchor># </span>Covariance Matrix Properties</h1></a><p>Consider a matrix of mean centred featurevectors
$$\Sigma \propto Z^TZ$$
The covariance matrix $\Sigma$ is directly proportional to the transposed matrix $Z^T$ multiplied by the original matrix $Z$</p><a href=#principle-axes-of-variation><h1 id=principle-axes-of-variation><span class=hanchor arialabel=Anchor># </span>Principle Axes of Variation</h1></a><a href=#basis><h2 id=basis><span class=hanchor arialabel=Anchor># </span>Basis</h2></a><p>A set of n linearly independent vectors in an n dimensional space</p><ul><li>Vectors are orthogonal</li><li>They form a &ldquo;coordinate system&rdquo;</li><li>There are an infinite number of possible basis</li></ul><a href=#the-first-principal-axis><h2 id=the-first-principal-axis><span class=hanchor arialabel=Anchor># </span>The First Principal Axis</h2></a><p>For a given set of n dimensional data, the first principle axis (or principal axis) is the vector that describes the direction of greatest variance
<img src=https://remnote-user-data.s3.amazonaws.com/uhMsBAdcESitpERI71YUhHnYOpm6QnhoHs-MF_1qFmw9CMTJPmsBduKPx1_DW858jzBPu-HoSC2lFK-M7npiPC645QHOuree3ecdgVZc9Std3DgKEr1SPuabjIS22gwI.png width=400 alt=|400></p><a href=#the-second-principal-axis><h2 id=the-second-principal-axis><span class=hanchor arialabel=Anchor># </span>The Second Principal Axis</h2></a><p>The second principal axis is a vector in the direction of greatest variance orthogonal (perpendicular) to the first major axis
<img src=https://remnote-user-data.s3.amazonaws.com/RnGLZ2BrSw3A2sY-vCuU8mfQPsJoDUz45fJ-OKbwJ8S6n5rvMlQegDD0TB_t1SG4cvQVeNwpFQ7TqkfDYbo4HWkLHSwjJhFAsCuALb0CwFRgvCVzCW3OfGHfOXYE6zkr.png width=400 alt=|400></p><a href=#the-third-principal-axis><h2 id=the-third-principal-axis><span class=hanchor arialabel=Anchor># </span>The Third Principal Axis</h2></a><p>In spaces with 3 or more dimensions, the third principle axis is the direction of greatest variance orthogonal to both the first and second principal axes
The set of n principal axes of an n dimensional space are a basis</p><a href=#eigenvectors-and-eigenvalues><h1 id=eigenvectors-and-eigenvalues><span class=hanchor arialabel=Anchor># </span>Eigenvectors and Eigenvalues</h1></a><p>$$\colorbox{aqua}A \colorbox{pink}v = \colorbox{lightgreen}{Î»} \colorbox{pink}v$$
Where</p><ul><li>$\colorbox{pink}{v}$ - An n dimensional vector, known as an eigenvector</li><li>$\colorbox{lightgreen}Î»$ - A scalar value, known as an eigenvalue</li><li>$\colorbox{aqua}A$ - An n\times n square matrix</li></ul><p>There are at most $n$ eigenvector-eigenvalue pairs
If A is symmetric, then the set of eigenvectors is orthogonal</p><ul><li>The eigenvector corresponding to the largest eigenvalue is the first principle component</li><li>Therefore, if A is a covariance matrix, then the eigenvectors are the principal axes</li><li>The eigenvalues are proportional to the variance of the data along each eigenvector</li></ul><a href=#finding-eigenvectors-and-eigenvalues><h1 id=finding-eigenvectors-and-eigenvalues><span class=hanchor arialabel=Anchor># </span>Finding Eigenvectors and Eigenvalues</h1></a><p>For small matrices (n\leq 4) there are algebraic solutions to finding all the eigenvector-eigenvalue pairs
For larger matrices, numerical solutions to the Eigendecomposition must be sought</p><a href=#eigendecomposition><h1 id=eigendecomposition><span class=hanchor arialabel=Anchor># </span>Eigendecomposition</h1></a><p>Breaks a matrix down into a matrix Q, a diagonal eigenvalue matrix \Lambda (only has values along the diagonal) and the inverse of the first matrix $Q^{-1}$
$$A=Q\Lambda Q^{-1}$$
If A is real symmetric (i.e. a covariance matrix), then Q^{-1}=Q^T (i.e. eigenvectors are orthogonal), therefore
$$A=Q\Lambda Q^T$$
So the eigendecomposition of a covariance matrix $A$ ($A=Q\Lambda Q^T$) gives you the principal axes and their relative magnitudes</p><a href=#ordering><h2 id=ordering><span class=hanchor arialabel=Anchor># </span>Ordering</h2></a><p>Some solvers are optimised to only find the top k eigenvalues and corresponding eigenvectors, rather than all of them
Standard eigendecomposition solver implementations will order the eigenvectors (columns of Q) such that the eigenvalues (in the diagonal of \Lambda) are sorted in order of decreasing value</p><a href=#principal-component-analysis><h1 id=principal-component-analysis><span class=hanchor arialabel=Anchor># </span>Principal Component Analysis</h1></a><a href=#linear-transform><h2 id=linear-transform><span class=hanchor arialabel=Anchor># </span>Linear Transform</h2></a><p>The effects of a linear transform can be reversed if W is invertible
$$Z=TW^{-1}$$
This is a lossy process if the dimensionality of the spaces is different
A linear transform W projects data from one space into another
$$T=ZW$$
T can have fewer dimensions than Z
Where original data is stored in the rows of Z</p><p><img src=https://remnote-user-data.s3.amazonaws.com/oFScSOCtwOE-e_tKPq7rmZjxSk-yRxPsv5ePsgDyhn5FPYQn_e_lt4KECL0JLzKMj9SRW0sn25o3zjfw-KzX_G1jWeZAmwroCs--nqyXwvjNLZPo2smc6Z040ubnQU7B.png width=400 alt=|400></p><p><img src=https://remnote-user-data.s3.amazonaws.com/fYsdP0JJlZ03kGEGvFamo2GcGx3wMWAPFzROFDCIapsAhfOFIS70xf4wyrrzxqYIonYdlq7vfxrZprmAThjldz_MXOp2L_0chhTZj32JnEmMZJFL5a5grI6UC8n8J74u.png width=400 alt=|400></p><a href=#pca><h2 id=pca><span class=hanchor arialabel=Anchor># </span>PCA</h2></a><p>PCA is an Orthogonal Linear Transform that maps data from its original space to a space defined by the principal axes of the data
Dimensionality reduction can be achieved by removing the eigenvectors with low eigenvalues from Q
i.e. Keeping the first L columns of Q, assuming the eigenvectors are sorted by decreasing eigenvalue
The transform matrix W is just the eigenvector matrix Q from the eigendecomposition of the covariance matrix of the data
Maps data from one dimension into another</p><a href=#pca-algorithm><h3 id=pca-algorithm><span class=hanchor arialabel=Anchor># </span>PCA Algorithm</h3></a><ol><li>Project the original vectors into a lower dimensional space $T_L$
$$T_L=ZQ_L$$</li><li>Sort the columns of Q and the corresponding diagonal values of \Lambda so that the eigenvalues are decreasing</li><li>Select the L largest eigenvectors of Q (the first L columns) to create the transform matrix $Q_L$</li><li>Form the vectors into a matrix Z, such that each row corresponds to a vector</li><li>Perform the eigendecomposition of the matrix $Z^TZ$, to recover the eigenvector matrix Q and diagonal eigenvalue matrix $\Lambda$
$$Z^TZ=Q\Lambda Q^T$$</li><li>Mean-centre the data vectors</li></ol><a href=#eigenfaces-eigenimages><h1 id=eigenfaces-eigenimages><span class=hanchor arialabel=Anchor># </span>Eigenfaces (Eigenimages)</h1></a><p>Flatten a grey level image into a vector</p><ul><li>Take each row and add it to the vector
<img src=https://remnote-user-data.s3.amazonaws.com/RhCsaBmFGEPl3qN5wLBAW_UBjASkWSKB18VSZ3nDx_ml0cVcXTE5t3_iBZ8XNcwTTcKOVq_B4-nggF3DVlDIjmBqJzYpVSSxI8U5quvKSfCZq0hpHKfBofZitv_ZQMRG.png width=400 alt=|400>
This causes some problems</li><li>Highly susceptible to image noise</li><li>It is not invariant to<ul><li>Change in position/orientation/scale of the object in the image</li><li>Changes in lighting</li><li>Size of the image</li></ul></li></ul><a href=#making-it-invariant><h2 id=making-it-invariant><span class=hanchor arialabel=Anchor># </span>Making it invariant</h2></a><p>Align (rotate, scale and translate) the images so that a common feature is in the same place</p><ul><li>i.e. The eyes in a set of face images
Require almost the same object pose across images</li><li>i.e. Faces facing the front
Make all the aligned images the same size
Optionally - Normalise (or histogram equalise) the images so they are invariant to global intensity changes</li></ul><a href=#another-problem-is-the-size-of-the-featurevectors><h2 id=another-problem-is-the-size-of-the-featurevectors><span class=hanchor arialabel=Anchor># </span>Another problem is the size of the featurevectors</h2></a><p>A potential solution is to apply PCA</p><ul><li>Fewer dimensions makes applying machine learning much more tractable</li><li>PCA can be used to reduce the dimensionality</li><li>A smaller number of dimensions allows greater robustness to noise and mis-alignment<ul><li>There are fewer degrees of freedom, so noise/mis-alignment has much less effect</li><li>And dominant features are captured
If the images are 100x200 pixels, the vector has 20000 dimensions
This is impractical and the vectors are highly susceptible to image noise and variations due to slight mis-alignments</li></ul></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://ik1g19.github.io/UoS-Notes/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Isaac Klugman using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2023</p><ul><li><a href=https://ik1g19.github.io/UoS-Notes/>Home</a></li><li><a href=https://twitter.com/_jzhao>Twitter</a></li><li><a href=https://github.com/jackyzha0>GitHub</a></li></ul></footer></div></div></body></html>