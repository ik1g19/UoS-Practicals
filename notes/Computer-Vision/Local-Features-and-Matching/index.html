<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Local Features Interest points are detected in the image and a feature is extracted from the surrounding pixels, represented by, usually just one, featurevector Feature points are used for"><meta property="og:title" content="Local Features and Matching"><meta property="og:description" content="Local Features Interest points are detected in the image and a feature is extracted from the surrounding pixels, represented by, usually just one, featurevector Feature points are used for"><meta property="og:type" content="website"><meta property="og:image" content="https://ik1g19.github.io/UoS-Notes/icon.png"><meta property="og:url" content="https://ik1g19.github.io/UoS-Notes/notes/Computer-Vision/Local-Features-and-Matching/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="Local Features and Matching"><meta name=twitter:description content="Local Features Interest points are detected in the image and a feature is extracted from the surrounding pixels, represented by, usually just one, featurevector Feature points are used for"><meta name=twitter:image content="https://ik1g19.github.io/UoS-Notes/icon.png"><meta name=twitter:site content="_jzhao"><title>Local Features and Matching</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://ik1g19.github.io/UoS-Notes//icon.png><link href=https://ik1g19.github.io/UoS-Notes/styles.80333fa2099c0bee674efa435fde378c.min.css rel=stylesheet><link href=https://ik1g19.github.io/UoS-Notes/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://ik1g19.github.io/UoS-Notes/js/darkmode.fdbb50a651b073e7d85910aebde4469d.min.js></script>
<script src=https://ik1g19.github.io/UoS-Notes/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script async src=https://ik1g19.github.io/UoS-Notes/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://ik1g19.github.io/UoS-Notes/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://ik1g19.github.io/UoS-Notes/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://ik1g19.github.io/UoS-Notes/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://ik1g19.github.io/UoS-Notes/",fetchData=Promise.all([fetch("https://ik1g19.github.io/UoS-Notes/indices/linkIndex.4cd7744b37da745103d2e306cd2cd001.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://ik1g19.github.io/UoS-Notes/indices/contentIndex.a1e306a3e6a8cedcc8275a04564ca752.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://ik1g19.github.io/UoS-Notes",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://ik1g19.github.io/UoS-Notes",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/ik1g19.github.io\/UoS-Notes\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=ik1g19.github.io/UoS-Notes src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://ik1g19.github.io/UoS-Notes/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://ik1g19.github.io/UoS-Notes/>My UoS Notes</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Local Features and Matching</h1><p class=meta>Last updated
Apr 12, 2023
<a href=https://github.com/ik1g19/notes/Computer%20Vision/Local%20Features%20and%20Matching.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#issues-with-this-method>Issues with this method</a><ol><li><a href=#problem-1>Problem 1</a></li><li><a href=#problem-2>Problem 2</a></li></ol></li></ol><ol><li><a href=#narrow-baseline-stereo>Narrow-baseline stereo</a></li><li><a href=#wide-baseline-stereo>Wide-baseline stereo</a></li></ol><ol><li><a href=#descriptor-requirements>Descriptor Requirements</a><ol><li><a href=#narrow-baseline>Narrow Baseline</a></li><li><a href=#wide-baseline>Wide Baseline</a></li></ol></li></ol><ol><li><a href=#narrow-baseline-template-matching>Narrow Baseline Template Matching</a><ol><li><a href=#interest-point-in-two-images-with-a-slight-change-in-position>Interest point in two images with a slight change in position</a></li><li><a href=#local-search-windows-based-on-the-interest-point-in-the-first-image>Local search windows, based on the interest point in the first image</a></li><li><a href=#the-template-can-then-be-matched-against-target-interest-points-in-the-second-image>The template can then be matched against target interest points in the second image</a></li></ol></li><li><a href=#problems-with-wider-baselines>Problems with Wider Baselines</a></li></ol><ol><li><a href=#use-local-histograms-instead-of-pixel-patches>Use local histograms instead of pixel patches</a></li><li><a href=#problems>Problems</a></li><li><a href=#overcoming-localisation-sensitivity>Overcoming Localisation Sensitivity</a></li><li><a href=#overcoming-lack-of-illumination-invariance>Overcoming Lack of Illumination Invariance</a><ol><li><a href=#local-gradient-histograms>Local Gradient Histograms</a></li></ol></li></ol><ol><li><a href=#sampling>Sampling</a></li><li><a href=#binning>Binning</a></li><li><a href=#weighting>Weighting</a></li></ol><ol><li><a href=#euclidean-matching>Euclidean Matching</a></li><li><a href=#improving-matching-performance>Improving Matching Performance</a></li></ol></nav></details></aside><a href=#local-features><h1 id=local-features><span class=hanchor arialabel=Anchor># </span>Local Features</h1></a><p>Interest points are detected in the image and a feature is extracted from the surrounding pixels, represented by, usually just one, featurevector
Feature points are used for</p><ul><li>Robot navigation</li><li>Camera pose estimation and camera calibration</li><li>Motion tracking</li><li>3d reconstruction</li><li>Image alignment</li><li>Object recognition</li><li>Indexing and database retrieval</li></ul><a href=#matching-with-features><h1 id=matching-with-features><span class=hanchor arialabel=Anchor># </span>Matching with Features</h1></a><ul><li>Detect feature points in both images</li><li>Find corresponding pairs
<img src=https://remnote-user-data.s3.amazonaws.com/MgSq7AlBR3NJb86c8nrUBZ3FhQCyrJDYEWvEeyAeOqzzPEWWn0Nz_uSQeTjrUatItrdAeo9CLwSxjYDRKyqKOP-ChBfVT26iik46nCX-a0TSmELS_x4M6ZFMbtxVdYaq.png width=400 alt=|400></li></ul><a href=#issues-with-this-method><h2 id=issues-with-this-method><span class=hanchor arialabel=Anchor># </span>Issues with this method</h2></a><a href=#problem-1><h3 id=problem-1><span class=hanchor arialabel=Anchor># </span>Problem 1</h3></a><p><img src=https://remnote-user-data.s3.amazonaws.com/pXvwDOMvt5qv3xoROYc_l87nvsdzZF2CRRbECnxl0aQRhqWQOqd_MuSFB94Jlh0syfqpsZWw2rLG7gECHVt4SiRA4Tbbn3vsib8E1b30Virp8WGI8p1gVnNBXE-tdXXC.png width=400 alt=|400></p><ul><li>We need to detect the same points independently in both images</li><li>We need a repeatable detector</li></ul><a href=#problem-2><h3 id=problem-2><span class=hanchor arialabel=Anchor># </span>Problem 2</h3></a><ul><li>We need an invariant, robust and distinctive descriptor of each point</li><li>For each point, we need to able to recognise the corresponding point in the other image
<img src=https://remnote-user-data.s3.amazonaws.com/g9QJxvqJocrDxgQmeVjzb3KVnXIyidb5GvIOETOw3kx5QdI3MJ-D-6Qz2bkwC8siIbiLAbwRnVNiUqABPbP9L78ymfoY5mZdk6bsFBx7zDbwRkBwIOLW3zjRFvBKD9Nh.png width=400 alt=|400></li></ul><a href=#two-types-of-matching-problem><h1 id=two-types-of-matching-problem><span class=hanchor arialabel=Anchor># </span>Two Types of Matching Problem</h1></a><p>In stereo vision there are two important concepts related to matching</p><a href=#narrow-baseline-stereo><h2 id=narrow-baseline-stereo><span class=hanchor arialabel=Anchor># </span>Narrow-baseline stereo</h2></a><p>The camera has made a small movement between its two images of the recorded object
<img src=https://remnote-user-data.s3.amazonaws.com/YauNt3JRUpF3yu4YIUEkiGEEnQ0WBARSUISVAyEoeFmai-Jevi5tIiM4GUQXBYQ4laRAV0CA5o9NJ-xFpFhgCuOAxnX1K_2dgfZ1A-KDFf9xuYjCfmDA9F3bBz6VfnzA.png width=400 alt=|400></p><a href=#wide-baseline-stereo><h2 id=wide-baseline-stereo><span class=hanchor arialabel=Anchor># </span>Wide-baseline stereo</h2></a><p><img src=https://remnote-user-data.s3.amazonaws.com/WepGxv2LlL5KbVUDi6liIcwWmC0uECnwuCXKQbRcm1Q5_isLnE9D6DtXYh1Z7scTz0Gw8Oo372tRovAG478BPPRxJGaWovl3ws2dSMVcCqHJ6YyY1CSxEekDjo_qyaPA.png width=400 alt=|400>
The camera has made a large movement between its two images of the recorded object</p><ul><li>These concepts extend to general matching<ul><li>Techniques for wide-baseline stereo are applicable to generic matching tasks<ul><li>e.g. Object recognition, panoramas, etc</li></ul></li><li>Techniques for narrow-baseline stereo are applicable to tracking where the object doesn&rsquo;t move too much between frames</li></ul></li></ul><a href=#robust-local-description><h1 id=robust-local-description><span class=hanchor arialabel=Anchor># </span>Robust Local Description</h1></a><a href=#descriptor-requirements><h2 id=descriptor-requirements><span class=hanchor arialabel=Anchor># </span>Descriptor Requirements</h2></a><a href=#narrow-baseline><h3 id=narrow-baseline><span class=hanchor arialabel=Anchor># </span>Narrow Baseline</h3></a><ul><li>Robustness to rotation and lighting are not so important</li><li>Descriptiveness can be reduced as search is over a smaller area</li></ul><a href=#wide-baseline><h3 id=wide-baseline><span class=hanchor arialabel=Anchor># </span>Wide Baseline</h3></a><ul><li>Need to be robust to intensity change, invariant to rotation</li><li>Robust to small localisation errors of the interest point<ul><li>The descriptor should not change too much if we move it by a few pixels, but to change more rapidly once we move further away</li></ul></li><li>Need to be highly descriptive to avoid mismatches<ul><li>But not so distinctive you can&rsquo;t find any matches</li></ul></li></ul><a href=#matching-by-correlation-template-matching><h1 id=matching-by-correlation-template-matching><span class=hanchor arialabel=Anchor># </span>Matching by Correlation (Template Matching)</h1></a><a href=#narrow-baseline-template-matching><h2 id=narrow-baseline-template-matching><span class=hanchor arialabel=Anchor># </span>Narrow Baseline Template Matching</h2></a><a href=#interest-point-in-two-images-with-a-slight-change-in-position><h3 id=interest-point-in-two-images-with-a-slight-change-in-position><span class=hanchor arialabel=Anchor># </span>Interest point in two images with a slight change in position</h3></a><p><img src=https://remnote-user-data.s3.amazonaws.com/hC87RA5XrBKiKxVkYJL7DC6YYWJeTF3ynedwDe-evZIrR55TbocCk5IzEQtAj7WSULvsWB5aPhJImNBsqPWVkdxEQPRmLQexyksW7Y1L8KIOi96K6a7JzFvRShM-PrCb.png width=400 alt=|400></p><a href=#local-search-windows-based-on-the-interest-point-in-the-first-image><h3 id=local-search-windows-based-on-the-interest-point-in-the-first-image><span class=hanchor arialabel=Anchor># </span>Local search windows, based on the interest point in the first image</h3></a><p><img src=https://remnote-user-data.s3.amazonaws.com/loMkCqQe3t6z76j-81Vc05xFKoUbd0oYznxHAuwPL4AOQcNUKCp3mgF-R8kpWim_m7ohHkSwCEqEQOz6zYpLMKoDVbnCQ2418xxEB99Gbd7IOHYl2mJX7mixUw1iL_yW.png width=400 alt=|400></p><a href=#the-template-can-then-be-matched-against-target-interest-points-in-the-second-image><h3 id=the-template-can-then-be-matched-against-target-interest-points-in-the-second-image><span class=hanchor arialabel=Anchor># </span>The template can then be matched against target interest points in the second image</h3></a><p><img src=https://remnote-user-data.s3.amazonaws.com/lXIvoWAFeuwJ1bdmkLcPrKw_FDxM2zocurvfBIkH-Qz048vmvf972Ezlg-577J9XoxKpAW9P09jxoxHFzT61RB2YYa64d1h8q7H2h8s7K3afvjF42mM4vDILLZz8YUxn.png width=400 alt=|400></p><a href=#problems-with-wider-baselines><h2 id=problems-with-wider-baselines><span class=hanchor arialabel=Anchor># </span>Problems with Wider Baselines</h2></a><ul><li>Sensitive to localisation of interest point<ul><li>Not a problem with a small search window</li></ul></li><li>Not robust to rotation</li><li>Can&rsquo;t assume a search area with wider baselines<ul><li>Need to consider all the interest points in the second image<ul><li>More likely to mismatch</li></ul></li></ul></li></ul><a href=#local-intensity-histograms><h1 id=local-intensity-histograms><span class=hanchor arialabel=Anchor># </span>Local Intensity Histograms</h1></a><a href=#use-local-histograms-instead-of-pixel-patches><h2 id=use-local-histograms-instead-of-pixel-patches><span class=hanchor arialabel=Anchor># </span>Use local histograms instead of pixel patches</h2></a><ol><li>Describe the region around each interest point with a pixel histogram
<img src=https://remnote-user-data.s3.amazonaws.com/BdtcXa7Kb81ptkrLcloRZ6BIZ-FkOWdvM11e9DNxixjl8fHr1PCmYyZ8cn9M5yG3Qtau7_xSbuKSTq14690JyNYZDsTfIkCmZWU9v634tyx2XCfEvEaP6I5kQUi7rImP.png width=400 alt=|400></li><li>Match each interest point in the first image to the most similar point in the second image<ul><li>i.e. In terms of Euclidean distance or some other measure between the histograms
<img src=https://remnote-user-data.s3.amazonaws.com/0GhfQcOe1cNcsG486ptg6GOaNnnpAsElQ3TMWaLibaE9va7XNCwM3WEFTfNa3FSJ0jL_6y8J9mT9J_FEFL5IZtGO4oBixjFR5bJNCzJ8bwHrVTnVUMTshXEhPLjxIgCC.png width=400 alt=|400></li></ul></li></ol><a href=#problems><h2 id=problems><span class=hanchor arialabel=Anchor># </span>Problems</h2></a><ul><li>Not invariant to illumination changes</li><li>Sensitive to interest point localisation</li><li>Not rotation invariant if the sampling window is square or rectangular<ul><li>Can be overcome by using a circular window</li></ul></li><li>Not necessarily very distinctive<ul><li>Many interest points are likely to have similar distribution of grey values</li></ul></li></ul><a href=#overcoming-localisation-sensitivity><h2 id=overcoming-localisation-sensitivity><span class=hanchor arialabel=Anchor># </span>Overcoming Localisation Sensitivity</h2></a><p>Apply a weighting so that pixels near the edge of the sampling patch have less effect, and those nearer the interest point have more</p><ul><li>Common to use a Gaussian weighting centred on the interest point for this
We want to allow the interest point to move a few pixels in any direction without changing the descriptor</li></ul><a href=#overcoming-lack-of-illumination-invariance><h2 id=overcoming-lack-of-illumination-invariance><span class=hanchor arialabel=Anchor># </span>Overcoming Lack of Illumination Invariance</h2></a><p>Illumination invariance is potentially achievable by normalising or equalising the pixel patches before constructing the histogram</p><a href=#local-gradient-histograms><h3 id=local-gradient-histograms><span class=hanchor arialabel=Anchor># </span>Local Gradient Histograms</h3></a><p>Instead of building histograms of the raw pixel values, we could instead build histograms that encode the gradient magnitude and direction for each pixel in the sampling patch</p><ul><li>The gradient magnitude and direction histogram is also more distinctive</li><li>Gradient magnitudes and directions are invariant to brightness change</li></ul><a href=#building-gradient-histograms><h4 id=building-gradient-histograms><span class=hanchor arialabel=Anchor># </span>Building Gradient Histograms</h4></a><ul><li>For each pixel in the sampling patch<ul><li>Accumulate the gradient magnitude of that pixel in the respective orientation bin</li></ul></li><li>Quantise the directions (0°-360°) into a number of bins<ul><li>Usually around 8 bins
<img src=https://remnote-user-data.s3.amazonaws.com/2ApfIGZrYEZC1Yra_ppqXtWJnZ7xbCIEiu1PV2hmi6WvWY5dIyW6j-iC6nFAHjrAmUEDkUipuqNR_3Dx6DRUaExJqRSe9Nra-YV5saOPUB5IFwxsiTCUTQXNqUzo55CN.png width=400 alt=|400>
You can compute the gradient orientations/directions and magnitudes of an image by using its partial derivatives</li><li><img src=https://remnote-user-data.s3.amazonaws.com/jvLgv-gF2sU-0fvyNAnegWsEU8-Jkzk9aqvjHdVehahAYSbn_eu-1wcMYglFJBB0NFq3VpY92nX7I2VEORserATfFjrkKZCwZGgTCFToY-lH-JmACGKUn54oV-z2debO.png width=400 alt=|400></li></ul></li></ul><a href=#rotation-invariance><h4 id=rotation-invariance><span class=hanchor arialabel=Anchor># </span>Rotation Invariance</h4></a><ul><li>By finding the dominant orientation<ul><li>And cyclically shifting the histogram so the dominant orientation is in the first bin
<img src=https://remnote-user-data.s3.amazonaws.com/y9g-0vm-wkAkYyhZW7X98YsDoliicRCiz_6MfYL09Y8kChpyt35BFtIREyV_iQN3u4mGyZNqiS87sTnmk-V0Dw-eukKjrXEAiY3_ikEveVHPtckq6Or87FyPZ3sIcZq-.png width=400 alt=|400></li></ul></li><li>Gradient histograms are not naturally rotation invariant</li><li>But can be made invariant</li></ul><a href=#the-sift-feature><h1 id=the-sift-feature><span class=hanchor arialabel=Anchor># </span>The SIFT Feature</h1></a><p>Standard SIFT geometry appends a spatial 4x4 grid of histograms with 8 orientations
Leading to a 128-dimensional feature</p><ul><li><p>Which is highly discriminative and robust</p></li><li><p>Built on local gradient histogram idea</p><ul><li>Which effectively creates multiple gradient histograms about the interest point and appends them all together into a longer feature</li></ul></li><li><p>Incorporates spatial binning</p></li><li><p>The Scale Invariant Feature Transform (SIFT) is widely used</p></li></ul><a href=#sift-construction><h1 id=sift-construction><span class=hanchor arialabel=Anchor># </span>SIFT Construction</h1></a><a href=#sampling><h2 id=sampling><span class=hanchor arialabel=Anchor># </span>Sampling</h2></a><ul><li>Sampling patch around interest point</li><li>Patch is either fixed size, or more commonly, proportional to the scale of the interest point
<img src=https://remnote-user-data.s3.amazonaws.com/5P2OQ570ccA_sUDgzsqDCBlcT1nZiSRg6eyTE9189-eqCQdIVricXJ0g7pJ61PVZQJs61fNkxKEb2HMQaYBKO59mFbLO7eIUJ69AC6yYI_1NEtdZuP2Op9VSKb74XdlS.png width=200 alt=|200></li></ul><a href=#binning><h2 id=binning><span class=hanchor arialabel=Anchor># </span>Binning</h2></a><p><img src=https://remnote-user-data.s3.amazonaws.com/4aF8Cqvz_c4eMkYMTTUTVS4Sv6FgqoLQbrxAlr5CRGUCrMPfVbcmpZ6s7WboDBi9O1yu_svtuzUjhqRyUXp4HBsPkR3HMszgGSFUAOjHHTWaB8fkXXGc-FWF1MrAEIrv.png width=200 alt=|200>
Gradient histograms, taking into account the gaussian weightings, are created for each spatial bin</p><ul><li>Orientations are measured relative to the overall dominant orientation of the patch
There are usually 4x4 spatial bins, rather than the 2x2 shown</li></ul><a href=#weighting><h2 id=weighting><span class=hanchor arialabel=Anchor># </span>Weighting</h2></a><p><img src=https://remnote-user-data.s3.amazonaws.com/O09a-SgiqIV1HKWV5_aTcJDeidpdxBX7Dqx7RunwyRvaZZ_eCU7rvnWKJo4ZCzS1NolHnQygiv2y-MocFabw4VcpavjCX4rY3cieYaykVGXqpy3xR1xXzqspFfghhWTk.png width=200 alt=|200>
The corners of the sampling square have zero weight, so in effect the sampling region is actually circular
A gaussian centred on the interest point weighs pixel contributions lower near the edges</p><a href=#matching-sift-features><h1 id=matching-sift-features><span class=hanchor arialabel=Anchor># </span>Matching SIFT Features</h1></a><a href=#euclidean-matching><h2 id=euclidean-matching><span class=hanchor arialabel=Anchor># </span>Euclidean Matching</h2></a><p>Take each feature in turn from the first image, and find the most similar feature in the second image</p><ul><li>Threshold can be used to reject poor matches</li><li>Doesn&rsquo;t work too well and results in lots of mismatches
The simplest way to match features</li></ul><a href=#improving-matching-performance><h2 id=improving-matching-performance><span class=hanchor arialabel=Anchor># </span>Improving Matching Performance</h2></a><p>Only form a match if the ratio of distances between the closest and second closest matches is less than a threshold</p><ul><li>Threshold typically set to 0.8</li><li>This means you can be more confident you have found the right point as you can see if there are any another similar points, that it could potentially be
Take each feature from the first image, and find the two closest features in the second image</li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://ik1g19.github.io/UoS-Notes/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Isaac Klugman using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://ik1g19.github.io/UoS-Notes/>Home</a></li><li><a href=https://twitter.com/_jzhao>Twitter</a></li><li><a href=https://github.com/jackyzha0>GitHub</a></li></ul></footer></div></div></body></html>